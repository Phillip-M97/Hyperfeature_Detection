{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the demo.** Run the following cells to compute semantic correspondences for real image pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import einops\n",
    "import math\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "from extract_hyperfeatures import load_models\n",
    "\n",
    "import archs.correspondence_utils\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet\\diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aaecfce559c4e249289af9468c5ef6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc4fe935da443b78556d562b45a7a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9741ea1ca9dc48a698c704dfcd0fc44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f55e79128845acaf99558f412de17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/pytorch_model.bin:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10018d48f88741e49cbf44afcaf1a9dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.bin:   0%|          | 0.00/1.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab04344268245c4987e3cd95dbbe15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffusion_mode: inversion\n",
      "idxs: [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2), (3, 0), (3, 1), (3, 2)]\n",
      "output_resolution: 64\n",
      "prompt: \n",
      "negative_prompt: \n"
     ]
    }
   ],
   "source": [
    "# Memory requirement is 13731MiB\n",
    "device = \"cuda\"\n",
    "config_path = \"configs/real.yaml\"\n",
    "config, diffusion_extractor, aggregation_network = load_models(config_path, device)\n",
    "\n",
    "def clear_cuda():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_pil, res=None, range=(-1, 1)):\n",
    "    if res:\n",
    "        image_pil = image_pil.resize(res, Image.BILINEAR)\n",
    "    image = torchvision.transforms.ToTensor()(image_pil) # range [0, 1]\n",
    "    r_min, r_max = range[0], range[1]\n",
    "    image = image * (r_max - r_min) + r_min # range [r_min, r_max]\n",
    "    return image[None, ...], image_pil\n",
    "\n",
    "def compute_pck(predicted_points, target_points, load_size, pck_threshold=0.1, target_bounding_box=None):\n",
    "    distances = np.linalg.norm(predicted_points - target_points, axis=-1)\n",
    "    if target_bounding_box is None:\n",
    "        pck = distances <= pck_threshold * max(load_size)\n",
    "    else:\n",
    "        left, top, right, bottom = target_bounding_box\n",
    "        pck = distances <= pck_threshold * max(right-left, bottom-top)\n",
    "    return distances, pck, pck.sum() / len(pck)\n",
    "\n",
    "def flatten_feats(feats):\n",
    "    b, c, w, h = feats.shape\n",
    "    feats = feats.view((b, c, -1))\n",
    "    feats = feats.permute((0, 2, 1))\n",
    "    return feats\n",
    "\n",
    "def normalize_feats(feats):\n",
    "    feats = feats / torch.linalg.norm(feats, dim=-1, keepdim=True)\n",
    "    return feats\n",
    "\n",
    "def points_to_idxs(points, load_size):\n",
    "    if not isinstance(points, np.ndarray):\n",
    "        points = np.array(points)\n",
    "\n",
    "    points_y = points[:, 0]\n",
    "    points_y = np.clip(points_y, 0, load_size[1] - 1)\n",
    "    points_x = points[:, 1]\n",
    "    points_x = np.clip(points_x, 0, load_size[0] - 1)\n",
    "    idx = load_size[1] * np.round(points_y) + np.round(points_x)\n",
    "    return idx.astype(np.int64)\n",
    "\n",
    "def load_and_compute_hyperfeats(image_path, device, load_size):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    proc_img, img_pil = process_image(img, res=load_size)\n",
    "    proc_img = proc_img.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            feats, _ = diffusion_extractor.forward(proc_img)\n",
    "            b, s, l, w, h = feats.shape\n",
    "            hyperfeats = aggregation_network(feats.float().view((b, -1, w, h)))\n",
    "            return hyperfeats\n",
    "\n",
    "def extract_hyperfeats_for_sources(json_path, image_dir, indices, device, load_size):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    hyperfeatures_dict = {}\n",
    "    points_dict = {}\n",
    "\n",
    "    for entry in data:\n",
    "        index = entry['image_path'].split('.')[0]\n",
    "        if int(index) in indices:\n",
    "            image_path = os.path.join(image_dir, entry['image_path'])\n",
    "            hyperfeatures = load_and_compute_hyperfeats(image_path, device, load_size)\n",
    "            hyperfeatures_dict[index] = hyperfeatures\n",
    "            points_dict[index] = entry['image_points']\n",
    "\n",
    "    return hyperfeatures_dict, points_dict\n",
    "\n",
    "def draw_correspondences_multi(source_images_paths, source_points_dict, predicted_points, target_image_path, title=\"\"):\n",
    "    num_sources = len(source_points_dict)\n",
    "    fig, axs = plt.subplots(1, num_sources + 1, figsize=(5 * (num_sources + 1), 5))  # Adjust subplot size dynamically\n",
    "\n",
    "    # Plot for each source points set\n",
    "    for idx, (key, source_image_path) in enumerate(source_images_paths.items()):\n",
    "        image = Image.open(source_image_path)\n",
    "        axs[idx].imshow(image)  # You might want to display the source image instead if available\n",
    "        axs[idx].set_title(f\"Source Points for Image {key}\")\n",
    "        points = np.array(source_points_dict[key])  # Ensure source_points is a numpy array\n",
    "        axs[idx].scatter(points[:, 0], points[:, 1], color='red', label='Source Points')\n",
    "        axs[idx].legend()\n",
    "\n",
    "    target_image = Image.open(target_image_path)\n",
    "\n",
    "    # Plot for predicted points\n",
    "    axs[-1].imshow(target_image)\n",
    "    axs[-1].set_title(\"Predicted Points on Target\")\n",
    "    axs[-1].scatter(predicted_points[:, 0], predicted_points[:, 1], color='blue', label='Predicted Points')\n",
    "    axs[-1].legend()\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eval_preds(predictions, ground_truths):\n",
    "    mae_per_point = []\n",
    "    mse_per_point = []\n",
    "    \n",
    "    for pred_points, gt_points in zip(predictions, ground_truths):\n",
    "        mae_per_image = []\n",
    "        mse_per_image = []\n",
    "        for pred_point, gt_point in zip(pred_points, gt_points):\n",
    "            mae_per_image.append(mean_absolute_error(gt_point, pred_point))\n",
    "            mse_per_image.append(mean_squared_error(gt_point, pred_point))\n",
    "        mae_per_point.append(mae_per_image)\n",
    "        mse_per_point.append(mse_per_image)\n",
    "    \n",
    "    # Average MAE and MSE per image\n",
    "    avg_mae_per_image = [np.mean(mae_image) for mae_image in mae_per_point]\n",
    "    avg_mse_per_image = [np.mean(mse_image) for mse_image in mse_per_point]\n",
    "\n",
    "    # Average MAE and MSE for the entire dataset\n",
    "    avg_mae_dataset = np.mean(avg_mae_per_image)\n",
    "    avg_mse_dataset = np.mean(avg_mse_per_image)\n",
    "\n",
    "    return {\n",
    "        'mae_per_point': mae_per_point,\n",
    "        'mse_per_point': mse_per_point,\n",
    "        'avg_mae_per_image': avg_mae_per_image,\n",
    "        'avg_mse_per_image': avg_mse_per_image,\n",
    "        'avg_mae_dataset': avg_mae_dataset,\n",
    "        'avg_mse_dataset': avg_mse_dataset\n",
    "    }\n",
    "    \n",
    "def calculate_errors(json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    total_mse = 0.0\n",
    "    total_mae = 0.0\n",
    "    num_entries = len(data)\n",
    "    \n",
    "    for entry in data:\n",
    "        total_mae += entry['mae']\n",
    "        total_mse += entry['mse']\n",
    "\n",
    "    avg_mae = total_mae/num_entries\n",
    "    avg_mse = total_mse/num_entries\n",
    "    \n",
    "    return avg_mae, avg_mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(archs.correspondence_utils)\n",
    "from archs.correspondence_utils import find_nn_multisource_correspondences2, draw_correspondences_multi\n",
    "\n",
    "def process_images(json_path, image_dir, source_indices, device, load_size, output_json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    load_size = tuple(load_size) if isinstance(load_size, list) else load_size\n",
    "\n",
    "    # Step 1: Define the source images and calculate their hyperfeatures\n",
    "    source_hyperfeats = {}\n",
    "    source_points = {}\n",
    "    source_image_paths = {}\n",
    "    for entry in data:\n",
    "        index = entry['image_path'].split('.')[0]\n",
    "        if int(index) in source_indices:\n",
    "            image_path = os.path.join(image_dir, entry['image_path'])\n",
    "            if os.path.exists(image_path):\n",
    "                hyperfeats = load_and_compute_hyperfeats(image_path, device, load_size)\n",
    "                source_hyperfeats[index] = hyperfeats\n",
    "                source_points[index] = entry['image_points']\n",
    "                source_image_paths[index] = image_path\n",
    "\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    evaluation_results = []\n",
    "\n",
    "\n",
    "    # Step 2: Iterate through the target images\n",
    "    for idx, target_entry in enumerate(data):\n",
    "        target_image_path = os.path.join(image_dir, target_entry['image_path'])\n",
    "        print(idx)\n",
    "        if os.path.exists(target_image_path):\n",
    "            target_hyperfeats = load_and_compute_hyperfeats(target_image_path, device, load_size)\n",
    "\n",
    "            # Process the target hyperfeatures\n",
    "            target_hyperfeats = torch.nn.functional.interpolate(target_hyperfeats, size=load_size, mode=\"bilinear\", align_corners=False)\n",
    "            target_hyperfeats = flatten_feats(target_hyperfeats)\n",
    "            target_hyperfeats = normalize_feats(target_hyperfeats)\n",
    "\n",
    "            max_sims = None\n",
    "            best_points = None\n",
    "\n",
    "            # Step 3: Iterate through the source images\n",
    "            for index in source_indices:\n",
    "                if str(index) in source_hyperfeats:\n",
    "                    img1_feats = source_hyperfeats[str(index)]\n",
    "                    source_points_arr = np.array(source_points[str(index)])\n",
    "\n",
    "                    # Process the source hyperfeatures\n",
    "                    img1_feats = torch.nn.functional.interpolate(img1_feats, size=load_size, mode=\"bilinear\", align_corners=False)\n",
    "                    img1_feats = flatten_feats(img1_feats)\n",
    "                    img1_feats = normalize_feats(img1_feats)\n",
    "\n",
    "                    # Get the indices of the source points\n",
    "                    source_idx = torch.from_numpy(points_to_idxs(source_points_arr, load_size)).long().to(device)\n",
    "                    img1_feats = img1_feats[:, source_idx, :]\n",
    "\n",
    "                    # Calculate the similarity matrix\n",
    "                    sims = torch.matmul(img1_feats, target_hyperfeats.permute((0, 2, 1)))\n",
    "\n",
    "                    if max_sims is None:\n",
    "                        max_sims = sims\n",
    "                        best_points = sims.argmax(dim=-1)\n",
    "                    else:\n",
    "                        current_max_values = sims.max(dim=-1).values\n",
    "                        better_mask = current_max_values > max_sims.max(dim=-1).values\n",
    "\n",
    "                        best_points = torch.where(better_mask, sims.argmax(dim=-1), best_points)\n",
    "                        max_sims = torch.where(better_mask.unsqueeze(-1), sims, max_sims)\n",
    "\n",
    "            # Convert the best points indices back into image coordinates\n",
    "            num_pixels = int(math.sqrt(sims.shape[-1]))\n",
    "\n",
    "            points_x = best_points % num_pixels\n",
    "            points_y = best_points // num_pixels\n",
    "            predicted_points = torch.stack([points_y, points_x], dim=-1)\n",
    "\n",
    "            initial_points = torch.from_numpy(np.array(source_points[str(source_indices[0])]))  # Just take the first source's points for reference\n",
    "            predicted_points = predicted_points[0]\n",
    "\n",
    "            predicted_points = predicted_points.cpu().numpy()\n",
    "            target_points = np.array(target_entry['image_points'])\n",
    "\n",
    "            predictions.append(predicted_points)\n",
    "            ground_truths.append(target_points)\n",
    "\n",
    "            distances, _, pck_metric = compute_pck(predicted_points, target_points, load_size)\n",
    "            \n",
    "            title = f\"Diffusion Hyperfeatures, Nearest Neighbors Matches \\nPCK@0.1: {pck_metric.round(decimals=2)}\"\n",
    "            target_image = Image.open(target_image_path)\n",
    "            draw_correspondences_multi(source_image_paths, source_points, predicted_points, target_image_path, title=title)\n",
    "            plt.show()\n",
    "        \n",
    "            evaluation_results.append({\n",
    "                'index': target_entry['image_path'],\n",
    "                'mae': mean_absolute_error(target_points, predicted_points),\n",
    "                'mse': mean_squared_error(target_points, predicted_points),\n",
    "                'pck': pck_metric.item() if isinstance(pck_metric, torch.Tensor) else pck_metric\n",
    "            })\n",
    "\n",
    "    with open(output_json_path, 'w') as outfile:\n",
    "            json.dump(evaluation_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cuda()\n",
    "image_path = \"assets/Biked_Subset\"\n",
    "json_path = \"annotations/geopoints_full_swapped_2.json\"\n",
    "load_size = (256, 256)\n",
    "output_size = (config[\"output_resolution\"], config[\"output_resolution\"])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_sourceimg = [1,2,74,1157, 1700]\n",
    "json_path_img = 'results_sourceimg1700.json'\n",
    "\n",
    "process_images(json_path, image_path, indices_sourceimg, device, load_size, json_path_img)\n",
    "calculate_errors(json_path_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diffusion_Hyperfeatures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
